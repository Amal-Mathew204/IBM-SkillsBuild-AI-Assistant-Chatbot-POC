import os
import requests
import torch
from peft import PeftModel
from transformers import AutoModelForCausalLM, AutoTokenizer

class LLMController:
    """
    Controller class for LLM interactions, focusing on response generation
    and conversation quality assessment for semantic search.
    """
    
    def __init__(self, model_path):
        """
        Initialize the LLM controller with a model.
        
        Args:
            model_path (str): Path to the pre-trained or fine-tuned model
        """
        base_model_id = "meta-llama/Llama-3.2-3B-Instruct"
        hf_token = "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"

        self.tokenizer = AutoTokenizer.from_pretrained(
            base_model_id,
            token=hf_token
        )
        base_model = AutoModelForCausalLM.from_pretrained(
            base_model_id,
            torch_dtype=torch.float16,
            low_cpu_mem_usage=True,
            token=hf_token
        )

        self.model = PeftModel.from_pretrained(base_model, model_path,
                                                      low_cpu_mem_usage=True)

    def process_conversation(self, conversation_history):
        """
        Process the conversation history, assess if it's suitable for semantic search,
        and only generate a response if it's NOT suitable.
        
        Args:
            conversation_history (list): List of message objects with role and content
            
        Returns:
            dict: Contains the assistant's response (or None if suitable) and whether
                 the conversation is suitable for semantic search
        """
        # Make a copy to avoid modifying the original
        conversation = conversation_history.copy()
        
        # First assess if the conversation is suitable for semantic search
        is_suitable, missing_info = self._assess_quality(conversation)
        
        # Only generate a response if the conversation is NOT suitable for search
        response = None
        if not is_suitable:
            response = self._generate_follow_up_questions(conversation, missing_info)
        else:
            response = 
        
        return {
            "response": response,
            "suitable_for_search": is_suitable
        }
    
    def _generate_response(self, conversation):
        """
        Generate a response to the latest user input in the conversation.
        
        Args:
            conversation (list): List of message objects with role and content
            
        Returns:
            str: The assistant's response
        """
        # Format the conversation for the model
        formatted_prompt = self.tokenizer.apply_chat_template(conversation, tokenize=False, add_generation_prompt=True)
        
        # Generate the response
        inputs = self.tokenizer(formatted_prompt, return_tensors="pt").to(self.model.device)
        with torch.no_grad():
            outputs = self.model.generate(
                inputs["input_ids"],
                max_new_tokens=256,
                do_sample=True,
                temperature=0.7,
                top_p=0.9
            )
        
        # Extract assistant's response
        output = self.tokenizer.decode(outputs[0], skip_special_tokens=False)
        assistant_reply = output.split("<|start_header_id|>assistant<|end_header_id|>")[-1].split("<|eot_id|>")[0].strip()
        
        return assistant_reply

    def _generate_follow_up_questions(self, conversation, missing_info):
        """
        Ask the model to generate a single follow-up question based on the conversation and missing info.
        
        Args:
            conversation (list): List of message objects with role and content
            missing_info (dict): The specific missing information that needs to be addressed
            
        Returns:
            str: A single follow-up question generated by the model
        """
        # Prepare the missing information context
        missing_info_str = ', '.join(missing_info)
        conversation_text = ' '.join([f"{turn['role']}: {turn['content']}" for turn in conversation])
        
        # If we have some context, we want to make the question more fluid with the ongoing conversation
        last_user_message = [turn['content'] for turn in conversation if turn['role'] == 'user'][-1]
        last_assistant_message = [turn['content'] for turn in conversation if turn['role'] == 'assistant'][-1] if any(turn['role'] == 'assistant' for turn in conversation) else ""

        # Prepare the prompt to ask the model to generate a specific follow-up question
        prompt = (
            "The following is a conversation between a user and an assistant. "
            "The assistant should ask a follow-up question that fits smoothly with the ongoing conversation, "
            "considering any missing information from the user. "
            "The question should be relevant and fluid based on the conversation so far. "
            "Only return the follow-up question with no additional text.\n\n"
            f"Conversation history:\n{conversation_text}\n\n"
            f"Last user message: {last_user_message}\n"
            f"Last assistant message: {last_assistant_message}\n"
            f"Missing information: {missing_info_str}\n\n"
            "Follow-up question:"
        )

        # Generate the follow-up question using the model
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.model.device)
        with torch.no_grad():
            outputs = self.model.generate(
                inputs["input_ids"],
                max_new_tokens=256,
                do_sample=True,
                temperature=0.7,
                top_p=0.9
            )
    
        # Extract the follow-up question from the output
        follow_up_question = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # Clean up the response to only include the follow-up question
        follow_up_question = follow_up_question.split("Follow-up question:")[-1].strip()
        follow_up_question = follow_up_question.replace("\n\n", "\n").strip()  # Remove excess spaces/newlines
        
        return follow_up_question

    def _assess_quality(self, conversation):
        """
        Assess if the conversation is suitable for semantic search by checking if all required information is provided.
        
        Args:
            conversation (list): List of message objects with role and content
            
        Returns:
            bool, dict: Whether the conversation is suitable for semantic search and a dictionary of missing information
        """
        # Extract user content from the conversation
        user_content = ' '.join([turn["content"].lower() for turn in conversation if turn["role"] == "user"])
        
        # Identify missing information
        missing_info = {}

        has_education = any(word in user_content for word in [
            "degree", "bsc", "msc", "phd", "university", "college", "education", "studied"
        ])
        if not has_education:
            missing_info["education"] = "Can you tell me about your education background?"

        has_career_goals = any(word in user_content for word in [
            "transition into", "become", "career", "job", "role", "industry", "position", "work as"
        ]) and not "not sure" in user_content and not "exploring options" in user_content
        if not has_career_goals:
            missing_info["career_goals"] = "What is your desired career role or industry?"

        has_knowledge = any(word in user_content for word in [
            "experience", "skills", "knowledge", "python", "java", "familiar", "proficient", "beginner", "advanced"
        ])
        if not has_knowledge:
            missing_info["knowledge"] = "What technical skills or programming languages are you familiar with?"

        # Check if off-topic
        is_off_topic = "weather" in user_content or "forecast" in user_content
        
        # Determine if content is suitable (all required info is available)
        content_suitable = has_education and has_career_goals and has_knowledge and not is_off_topic
        
        return content_suitable, missing_info

    def get_courses_from_semantic_search(context: str) -> list[dict] | None:
        """
        Method retrieves courses from semantic search container
        
        Args:
            context(str): String containing information to find a course for the user 
                          extracted information from the user conversation
        Returns:
            list[dict]: list of course objects
            None: Returned if No courses was obtained from the container
        """
        url: str = f"http://semanticsearch:{os.getenv('SEMANTIC_SEARCH_PORT')}/{context}/{5}"
        response = requests.get(url, timeout=30)
        print("Semantic Search Response: ")
        print(response.status_code)
        print(response.text)
        if response.status_code == 200:
            courses = response.json()
            return courses
        return None